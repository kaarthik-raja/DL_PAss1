{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--lr LR] [--momentum MOMENTUM]\n",
      "                             [--num_hidden NUM_HIDDEN] [--sizes SIZES]\n",
      "                             [--activation {sigmoid,tanh}] [--loss {sq,ce}]\n",
      "                             [--opt {gd,momentum,nag,adam}]\n",
      "                             [--batch_size BATCH_SIZE] [--epoch EPOCH]\n",
      "                             [--anneal {True,False}] [--save_dir SAVE_DIR]\n",
      "                             [--expt_dir EXPT_DIR] [--train TRAIN]\n",
      "                             [--test TEST] [--validation VALIDATION]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Windows\\AppData\\Roaming\\jupyter\\runtime\\kernel-1e18e1d7-1a79-431d-a43d-6709d4dab0be.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import argparse as agp\n",
    "import os \n",
    "from collections import Counter as freq_ \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "np.random.seed(1234)\n",
    "gamma=0.9\n",
    "eta=0.001\n",
    "adam_b1=0.9\n",
    "adam_bp1=0.9\n",
    "adam_b2=0.999\n",
    "adam_bp2=0.999\n",
    "adam_epsilon= 0.0000001\n",
    "ploss=0.0\n",
    "#initialize weights and bias\n",
    "wt=[] #list of weight matrices \n",
    "bias=[] #list of bias vectors\n",
    "\n",
    "momentum_w=[]\n",
    "momentum_b=[]\n",
    "tloss=[]\n",
    "vloss=[]\n",
    "\n",
    "\n",
    "look_w=[]\n",
    "look_b=[]\n",
    "update_w=[]\n",
    "update_b=[]\n",
    "\n",
    "adam_w_m=[]\n",
    "adam_w_v=[]\n",
    "adam_b_m=[]\n",
    "adam_b_v=[]\n",
    "def initwts():\n",
    "    global sizes,wt,bias\n",
    "    global momentum_w,momentum_b\n",
    "    global look_w,look_b\n",
    "    global adam_w_m,adam_w_v,adam_b_m,adam_b_v,adam_bp2,adam_bp1\n",
    "\n",
    "    sizes=np.asarray(sizes)\n",
    "    sizes=np.insert(sizes,0,350)\n",
    "    sizes=np.append(sizes,10).astype(int)\n",
    "\n",
    "    same_model = False\n",
    "    if os.path.exists(os.path.join(\"Model\",\"model.pkl\")):\n",
    "        with open(os.path.join(\"Model\",\"model.pkl\"), 'rb') as f:\n",
    "            model=pickle.load(f)\n",
    "            nsizes  = model[\"sizes\"]\n",
    "            adam_bp2 = model.get(\"adam_bp2\",adam_b2)\n",
    "            adam_bp1 = model.get(\"adam_bp1\",adam_b1)\n",
    "            same_model = True\n",
    "            if num_hidden == model[\"num_hidden\"]:\n",
    "                for k in range(num_hidden+1):\n",
    "                    if nsizes[k] != sizes[k]:\n",
    "                        same_model = False\n",
    "            else:\n",
    "                same_model = False\n",
    "            print(\"Same  Model :\",same_model)\n",
    "    if same_model:\n",
    "        wt = np.load(os.path.join(\"Model\",\"wt.npy\"))\n",
    "        bias = np.load(os.path.join(\"Model\",\"bias.npy\"))\n",
    "\n",
    "        print(\"continuing previous model data....\")\n",
    "    else:\n",
    "        wt= [None]*(num_hidden+1)\n",
    "        bias = [None]*(num_hidden+1)\n",
    "        for n in range(num_hidden+1):\n",
    "            w=np.random.randn(sizes[n],sizes[n+1])/np.sqrt(sizes[n]+sizes[n+1])\n",
    "            b=np.random.randn( sizes[n+1] )  \n",
    "            # w= np.subtract(np.multiply( np.random.rand(sizes[n],sizes[n+1]),2),1)\n",
    "            # b= np.subtract(np.random.rand(sizes[n+1]),0.5)\n",
    "            wt[n]=w\n",
    "            bias[n]=b\n",
    "\n",
    "    for n in range(num_hidden+1):\n",
    "        if opt ==\"momentum\":\n",
    "            momentum_w.append(np.zeros((sizes[n],sizes[n+1])))\n",
    "            momentum_b.append(np.zeros((sizes[n+1])))\n",
    "        elif opt == \"nag\":\n",
    "            look_w.append(wt[n])\n",
    "            look_b.append(b[n])\n",
    "            update_w.append(np.zeros((sizes[n],sizes[n+1])))\n",
    "            update_b.append(np.zeros((sizes[n+1])))\n",
    "        elif opt == \"adam\":\n",
    "            adam_w_m.append(np.zeros((sizes[n],sizes[n+1])))\n",
    "            adam_w_v.append(np.zeros((sizes[n],sizes[n+1])))\n",
    "            adam_b_m.append(np.zeros((sizes[n+1])))\n",
    "            adam_b_v.append(np.zeros((sizes[n+1])))\n",
    "    if same_model:\n",
    "        if opt == \"adam\" and os.path.exists(os.path.join(\"Model\",\"adam_w_v.npy\")):\n",
    "            adam_w_v = np.load(os.path.join(\"Model\",\"adam_w_v.npy\"))\n",
    "            adam_w_m = np.load(os.path.join(\"Model\",\"adam_w_m.npy\"))\n",
    "            adam_b_v = np.load(os.path.join(\"Model\",\"adam_b_v.npy\"))\n",
    "            adam_b_m = np.load(os.path.join(\"Model\",\"adam_b_m.npy\"))\n",
    "\n",
    "def fgrad(hs):\n",
    "    if activation == \"sigmoid\":\n",
    "        return np.multiply(hs,1-hs)\n",
    "    else:\n",
    "        return np.subtract(1,np.multiply(hs , hs))\n",
    "\n",
    "def fval(a):\n",
    "    if activation == \"sigmoid\":\n",
    "        return np.reciprocal(np.add(1,np.exp(np.negative(a) )))\n",
    "    else:\n",
    "        return np.multiply(np.subtract(np.exp(a),np.exp( np.negative(a) )),np.reciprocal( np.add(np.exp(a),np.exp( np.negative(a) )) ) )\n",
    "\n",
    "# def outputError(y,oneH):\n",
    "    # pass\n",
    "def optimizer(k,Dwk,Dbk):\n",
    "    global wt,bias,momentum_w,momentum_b\n",
    "    # print(\"optimizer\",opt,k,Dwk[1,1:4])\n",
    "    if opt == \"gd\":\n",
    "        wt[k]=np.subtract(wt[k],np.multiply(eta,Dwk))\n",
    "        bias[k]=np.subtract(bias[k],np.multiply(eta,Dbk))\n",
    "    elif opt == \"momentum\":\n",
    "        momentum_w[k]=np.multiply(momentum_w[k],gamma)\n",
    "        momentum_w[k]=np.add(momentum_w[k],np.multiply(eta,Dwk))\n",
    "        wt[k]=np.subtract(wt[k],momentum_w[k])        \n",
    "        momentum_b[k]=np.multiply(momentum_b[k],gamma)\n",
    "        momentum_b[k]=np.add(momentum_b[k], np.multiply(eta,Dbk))\n",
    "        bias[k]=np.subtract(bias[k], momentum_b[k])\n",
    "    elif opt == \"nag\":\n",
    "        update_w[k]= np.add( np.multiply(gamma,update_w[k]),np.multiply(eta,Dwk) )\n",
    "        look_w[k]= np.subtract(look_w[k],update_w[k])\n",
    "        update_b[k]= np.add( np.multiply(gamma,update_b[k]),np.multiply(eta,Dbk) )\n",
    "        look_b[k]= np.subtract(look_b[k],update_b[k])\n",
    "    elif opt == \"adam\":\n",
    "        adam_w_m[k] = np.add(np.multiply(adam_b1,adam_w_m[k]),np.multiply(1-adam_b1,Dwk))\n",
    "        adam_b_m[k] = np.add(np.multiply(adam_b1,adam_b_m[k]),np.multiply(1-adam_b1,Dbk))\n",
    "\n",
    "        adam_w_v[k] = np.add(np.multiply(adam_b2,adam_w_v[k]),np.multiply(1-adam_b2,np.multiply(Dwk,Dwk)))\n",
    "        adam_b_v[k] = np.add(np.multiply(adam_b2,adam_b_v[k]),np.multiply(1-adam_b2,np.multiply(Dbk,Dbk)))\n",
    "\n",
    "        wt[k] = np.subtract(  wt[k],  np.multiply( np.multiply(eta, np.reciprocal( np.sqrt( np.add( np.divide(adam_w_v[k],1-adam_bp2) ,adam_epsilon) )) ), np.divide(adam_w_m[k],1-adam_bp1) ) )\n",
    "        bias[k] = np.subtract(bias[k],np.multiply( np.multiply(eta, np.reciprocal( np.sqrt( np.add( np.divide(adam_b_v[k],1-adam_bp2) ,adam_epsilon) )) ), np.divide(adam_b_m[k],1-adam_bp1) ) )\n",
    "\n",
    "# implementing functions to do different tasks. This is the main function block\n",
    "#def vanilla_grad_desc(num_hidden,sizes):\n",
    "def grad_desc():\n",
    "    global freqClass,gloss,adam_bp1,adam_bp2\n",
    "    x=mini[0:,0:350]\n",
    "    x=np.divide(np.subtract(x.astype(float),127),128)\n",
    "    y=mini[0:,350]\n",
    "\n",
    "    hs=[]    \n",
    "    h=x\n",
    "    hs.append(h)\n",
    "\n",
    "    if opt == \"nag\":\n",
    "        for k in range(num_hidden+1):\n",
    "            # print(type(look_w),update_w[k])\n",
    "            wt[k] = np.subtract(look_w[k],np.multiply(gamma,update_w[k]) )\n",
    "            bias[k] = np.subtract(look_b[k],np.multiply(gamma,update_b[k]) )\n",
    "\n",
    "    #forward Propagation\n",
    "    for n in range(num_hidden):\n",
    "            a=np.add(np.matmul(h,wt[n]),bias[n])\n",
    "            h=fval(a)\n",
    "            hs.append(h)\n",
    "\n",
    "    a=np.add(np.matmul(h,wt[num_hidden]),bias[num_hidden]) \n",
    "    yhat = np.exp(a) / np.sum (np.exp(a),axis=1,keepdims=True)\n",
    "\n",
    "    loss = np.sum(-np.log(yhat[range(x.shape[0]),y.astype(int)]))\n",
    "    gloss +=loss\n",
    "    oneH = np.zeros((x.shape[0],10))\n",
    "    oneH[range(x.shape[0]),y.astype(int)]=1\n",
    "\n",
    "    yt = np.zeros((x.shape[0],10))\n",
    "    yt[range(x.shape[0]),np.argmax(yhat,axis=1)]=1\n",
    "\n",
    "    freqClass += np.sum(yt,axis=0)\n",
    "\n",
    "    nof= np.sum(np.multiply(yt,oneH))\n",
    "    nofc[iii]+=nof    \n",
    "\n",
    "\n",
    "    #backward Propagation\n",
    "    Dak = yhat -  oneH\n",
    "\n",
    "    for k in range(num_hidden,-1,-1):\n",
    "        Dwk = np.matmul(hs[k].T,Dak)\n",
    "        Dwk = Dwk/x.shape[0]\n",
    "        Dbk = Dak\n",
    "        Dbk = np.mean(Dbk,axis=0)\n",
    "        optimizer(k,Dwk,Dbk)\n",
    "        Dhk = np.matmul(Dak , wt[k].T)\n",
    "\n",
    "        Dak = np.multiply(Dhk,  fgrad(hs[k]) )       \n",
    "\n",
    "    if opt == \"adam\":\n",
    "        adam_bp1=adam_bp1*adam_b1\n",
    "        adam_bp2=adam_bp2*adam_b2\n",
    "    return loss\n",
    "\n",
    "def validation(data,classlbl=False):\n",
    "    global adam_bp1,adam_bp2,expt_dir,ploss,eta,anneal\n",
    "    x=data[0:,0:350]\n",
    "    x=np.divide(np.subtract(x.astype(float),127),128)\n",
    "\n",
    "    if not classlbl:\n",
    "        y=data[0:,350]\n",
    "\n",
    "    h=x\n",
    "    for n in range(num_hidden):\n",
    "            a=np.add(np.matmul(h,wt[n]),bias[n])\n",
    "            h=fval(a)\n",
    "\n",
    "    a=np.add(np.matmul(h,wt[num_hidden]),bias[num_hidden]) \n",
    "    yhat = np.exp(a) / np.sum (np.exp(a),axis=1,keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "    yt = np.zeros((x.shape[0],10))\n",
    "    yt[range(x.shape[0]),np.argmax(yhat,axis=1)]=1\n",
    "\n",
    "    freqClass = np.sum(yt,axis=0)\n",
    "    if classlbl:\n",
    "        print(\"test\",data.shape,freqClass)\n",
    "        return np.argmax(yhat,axis=1)\n",
    "\n",
    "    loss = np.sum(-np.log(yhat[range(x.shape[0]),y.astype(int)]))\n",
    "\n",
    "    if ploss < loss and anneal:\n",
    "        eta = eta*0.8 \n",
    "\n",
    "    oneH = np.zeros((x.shape[0],10))\n",
    "    oneH[range(x.shape[0]),y.astype(int)]=1\n",
    "\n",
    "    nof= np.sum(np.multiply(yt,oneH))\n",
    "    logfile(expt_dir,\"log_validation.txt\")\n",
    "    vloss.append(step,loss)\n",
    "\n",
    "    print(\"success:\" ,float(nof)/x.shape[0],\"\\n\",freqClass)\n",
    "\n",
    "\n",
    "def csv_list(string):\n",
    "   return [ int(i) for i in string.split(',')]\n",
    "\n",
    "def annealf(string):\n",
    "    if string in [\"true\",\"True\",\"T\",\"t\",\"1\" ] :\n",
    "        return True\n",
    "    elif string in [\"False\",\"false\",\"F\",\"f\",\"0\"]:\n",
    "        return False\n",
    "\n",
    "def logfile(expt_dir,log_type): #log_type: log_train.txt/log_validation.txt depending upon the data used\n",
    "    f_location='%s%s' %(expt_dir,log_type)\n",
    "    f=open(f_location , 'a+')\n",
    "    f.write(\" Epoch : %d , Step : %d , Loss : %f , Error: %f , lr :%f\" %(iii,step,gloss,(55000-nofc[iii])/55000,eta))\n",
    "    f.close()    \n",
    "\n",
    "def testprediction(expt_dir):\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    global lr,momentum,num_hidden,sizes,activation,loss,opt,batch_size,epoch,anneal,save_dir,expt_dir,train_path,test_path,valid_path\n",
    "    global train,test,valid,wt,bias,adam_bp1,adam_bp2,steps_per_batch\n",
    "    global iii,jj,mini,nofc,file,freqClass,gloss,step,train,test,valid,tloss,vloss\n",
    "    print(\"parsing...\")\n",
    "    parser = agp.ArgumentParser()\n",
    "    parser.add_argument(\"--lr\", type=float, help=\"the learning rate\", default=0.01)\n",
    "    parser.add_argument(\"--momentum\", type=float, help=\"the momentum in lr\", default=0.5)\n",
    "    parser.add_argument(\"--num_hidden\", type=int, help=\"# of Hidden Layers\", default=1)\n",
    "    parser.add_argument(\"--sizes\", type=csv_list, help=\"# of Nodes per H_Layer\", default= [50])\n",
    "    parser.add_argument(\"--activation\", type=str, help=\"activation function\", default= \"sigmoid\", choices=[\"sigmoid\",\"tanh\"])\n",
    "    parser.add_argument(\"--loss\", type=str, help=\"loss function\", default= \"ce\", choices=[\"sq\",\"ce\"])\n",
    "    parser.add_argument(\"--opt\", type=str, help=\"optimizer\", default= \"adam\", choices=[\"gd\",\"momentum\",\"nag\",\"adam\"])\n",
    "    parser.add_argument(\"--batch_size\", type=int, help=\"batch size per step\", default= 20)\n",
    "    parser.add_argument(\"--epoch\", type=int, help=\"# of Epochs\", default= 5000)\n",
    "    parser.add_argument(\"--anneal\", type=annealf, help=\"anneal\", default= False,choices=[True,False])\n",
    "    parser.add_argument(\"--save_dir\", type=str, help=\"Save dir location\", default= \"pa1\")\n",
    "    parser.add_argument(\"--expt_dir\", type=str, help=\"expt_dir location\", default= os.path.join(\"pa1\",\"exp1\"))\n",
    "    parser.add_argument(\"--train\", type=str, help=\"train file location\", default= os.path.join(\"Data\",\"train.csv\"))\n",
    "    parser.add_argument(\"--test\", type=str, help=\"test file location\", default= os.path.join(\"Data\",\"test.csv\"))\n",
    "    parser.add_argument(\"--validation\", type=str, help=\"validation file location\", default= os.path.join(\"Data\",\"valid.csv\"))\n",
    "    args=parser.parse_args()\n",
    "    lr,momentum=args.lr,args.momentum\n",
    "    num_hidden,sizes=args.num_hidden,args.sizes\n",
    "    activation,loss,opt=args.activation,args.loss,args.opt\n",
    "    batch_size,epoch=args.batch_size,args.epoch\n",
    "    anneal=args.anneal\n",
    "    save_dir,expt_dir=args.save_dir,args.expt_dir\n",
    "    train_path=args.train\n",
    "    test_path=args.test\n",
    "    valid_path=args.validation\n",
    "\n",
    "    file = open(\"train.txt\",\"w\")\n",
    "    train=pd.read_csv(train_path)\n",
    "\n",
    "    valid=pd.read_csv(valid_path)\n",
    "    print(\"finished reading images...\")\n",
    "\n",
    "    # train=train.values\n",
    "\n",
    "    train=train.values\n",
    "    valid=valid.values\n",
    "    x=train[:,1:785]\n",
    "    y=train[:,785]\n",
    "\n",
    "\n",
    "    if os.path.exists(os.path.join(\"Model\",\"PCA.sav\")):\n",
    "        print(\"PCA exists\")\n",
    "        pcamod = joblib.load(os.path.join(\"Model\",\"PCA.sav\"))\n",
    "    else:\n",
    "        pca=PCA(n_components=350)\n",
    "        pcamod=pca.fit(x) #pickle this to use it on test data\n",
    "        print(\"Dumping  PCA\")\n",
    "        joblib.dump(pcamod, os.path.join(\"Model\",\"PCA.sav\"))\n",
    "    x=pcamod.transform(x)\n",
    "\n",
    "\n",
    "    yn = y.reshape(x.shape[0],1)\n",
    "    train=  np.hstack((x,yn))\n",
    "\n",
    "\n",
    "    x_val=valid[:,1:785]\n",
    "    y_val =valid[:,785]\n",
    "    x_val =pcamod.transform(x_val)\n",
    "    yn = y_val.reshape(y_val.shape[0],1)\n",
    "    valid=  np.hstack((x_val,yn))\n",
    "\n",
    "    # print(\" Correctly classified samples ratio is %d \\n\"%round((nof/55000),2))\n",
    "    steps_per_batch=int(train.shape[0]/batch_size)\n",
    "    initwts()\n",
    "    nofc=np.zeros(epoch)\n",
    "\n",
    "    step=0 # update step for printing loss\n",
    "    for iii in range(epoch):\n",
    "        np.random.shuffle(train)\n",
    "        freqClass = np.zeros(10)\n",
    "        gloss=0\n",
    "        for jj in range(steps_per_batch):\n",
    "            step=step+1\n",
    "            if(step%100==0):\n",
    "                logfile(expt_dir,\"log_train.txt\")\n",
    "            mini = train[jj*batch_size:(jj+1)*batch_size,:]\n",
    "            lloss=grad_desc()\n",
    "            tloss.append(step,lloss)\n",
    "\n",
    "\n",
    "\n",
    "        if (iii%10)==0:\n",
    "            validation(valid)\n",
    "        if (iii %100) == 30:\n",
    "            with open(os.path.join(\"Model\",\"model.pkl\"), 'wb') as f:\n",
    "                model = {}\n",
    "                model[\"sizes\"]=sizes\n",
    "                model[\"num_hidden\"]=num_hidden\n",
    "                if opt ==\"adam\":\n",
    "                    model[\"adam_bp1\"]=adam_bp1\n",
    "                    model[\"adam_bp2\"]=adam_bp2\n",
    "                    np.save(os.path.join(\"Model\",\"adam_w_v.npy\"),np.array(adam_w_v))\n",
    "                    np.save(os.path.join(\"Model\",\"adam_b_v.npy\"),np.array(adam_b_v))\n",
    "                    np.save(os.path.join(\"Model\",\"adam_w_m.npy\"),np.array(adam_w_m))\n",
    "                    np.save(os.path.join(\"Model\",\"adam_b_m.npy\"),np.array(adam_b_m))\n",
    "\n",
    "                pickle.dump(model, f)\n",
    "                np.save(os.path.join(\"Model\",\"wt.npy\"),np.array(wt))\n",
    "                np.save(os.path.join(\"Model\",\"bias.npy\"),np.array(bias))\n",
    "\n",
    "        print(\"\\n \",iii,nofc[iii],freqClass.astype(int),gloss)\n",
    "        if gloss<80 and nofc[iii]==train.shape[0]:\n",
    "            break\n",
    "    # for i in range(10):\n",
    "        # yc=vanilla_grad_desc(num_hidden,sizes)\n",
    "\n",
    "    file.close()\n",
    "    # plt.pyplot(iii,nofc)\n",
    "\n",
    "    #test set prediction\n",
    "    test=pd.read_csv(test_path)\n",
    "    test=test.values\n",
    "    sno=test[:,0]\n",
    "    x=test[:,1:785]\n",
    "    x=pcamod.transform(x)\n",
    "    ypred=validation(x,classlbl=True)\n",
    "\n",
    "    fn=\"test_submission.csv\"\n",
    "    f_location  = os.path.join(expt_dir,fn)\n",
    "    fpred=open(f_location,'a+')\n",
    "    fpred.write(\"id,label\\n\")\n",
    "    for r in range(x.shape[0]):\n",
    "        fpred.write(\"%d,%d\\n\"%(sno[r],ypred[r]) )\n",
    "    # print(\"  \",ypred[2000:2500],sno[])\n",
    "    fpred.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
